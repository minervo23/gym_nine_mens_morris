{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-06T12:06:45.115476Z",
     "start_time": "2023-09-06T12:06:45.099086Z"
    }
   },
   "outputs": [],
   "source": [
    "from gym_nmm.nine_mens_morris import nmm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "env = nmm.env(render_mode='ansi')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T12:06:58.142627Z",
     "start_time": "2023-09-06T12:06:58.056016Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['player_0', 'player_1']\n",
      "Discrete(24) Box(0.0, 1.0, (72,), float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": "((72,),\n array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1]))"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "print(env.possible_agents)\n",
    "print(env.action_spaces['player_0'], env.observation_spaces['player_0'])\n",
    "obs = env.observe('player_0')\n",
    "obs['observation'].shape, obs['action_mask']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T12:07:00.467589Z",
     "start_time": "2023-09-06T12:07:00.448667Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import MeanMetric\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import one_hot"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T12:13:01.527680Z",
     "start_time": "2023-09-06T12:13:01.403318Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class AlphaZeroNMM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 76 = (24 pieces of current player) + (24 pieces of opponent) + (24, position of the lifted piece to move) + (4 action types: place_phase_1, lift_phase_2, drop_phase_2, kill)\n",
    "        self.v = nn.Sequential(\n",
    "            nn.Linear(76, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        self.p_place = nn.Sequential(\n",
    "            nn.Linear(76, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 24),\n",
    "        )\n",
    "\n",
    "        self.p_lift = nn.Sequential(\n",
    "            nn.Linear(76, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 24),\n",
    "        )\n",
    "\n",
    "        self.p_drop = nn.Sequential(\n",
    "            nn.Linear(76, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 24),\n",
    "        )\n",
    "\n",
    "        self.p_kill = nn.Sequential(\n",
    "            nn.Linear(76, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 24),\n",
    "        )\n",
    "\n",
    "    def forward(self, s, action_types):\n",
    "        # Add action types to observation because the value network should know which phase you're in to make it fully observable.\n",
    "        action_type_one_hot = one_hot(action_types, num_classes=4)  # (b, 4)\n",
    "        s = torch.cat((s, action_type_one_hot), dim=1)\n",
    "\n",
    "        v = self.v(s)  # (b, 1)\n",
    "        p_place = self.p_place(s)  # (b, 24)\n",
    "        p_lift = self.p_lift(s)  # (b, 24)\n",
    "        p_drop = self.p_drop(s)  # (b, 24)\n",
    "        p_kill = self.p_kill(s)  # (b, 24)\n",
    "\n",
    "        p = []\n",
    "        for i in range(len(s)):\n",
    "            if action_types[i] == nmm.ActionType.PLACE:\n",
    "                p.append(p_place[i])\n",
    "            elif action_types[i] == nmm.ActionType.LIFT:\n",
    "                p.append(p_lift[i])\n",
    "            elif action_types[i] == nmm.ActionType.DROP:\n",
    "                p.append(p_drop[i])\n",
    "            elif action_types[i] == nmm.ActionType.KILL:\n",
    "                p.append(p_kill[i])\n",
    "            else:\n",
    "                raise Exception(f'Action type must be 0, 1, 2 or 3.')\n",
    "        p = torch.stack(p)  # (b, 24)\n",
    "        p = torch.softmax(p, dim=1)\n",
    "\n",
    "        return p, v"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T12:13:07.058406Z",
     "start_time": "2023-09-06T12:13:06.982211Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MCTS\n",
    "\n",
    "Similar to AlphaZero's MCTS, but with following changes:\n",
    "\n",
    "1. Agents do not necessarily switch turns at every level.\n",
    "2. Limited number of visits for each node in case of cyclic state graph (loops).\n",
    "    https://github.com/jonathan-laurent/AlphaZero.jl/issues/47\n",
    "   a. This is taken care by the limit to the number of steps in the env."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class MCTS:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.q_sa = {}  # stores Q values for (s, a)\n",
    "        self.n_sa = {}  # stores # of times edge (s, a) was visited\n",
    "        self.p_s = {}  # stores initial policy returned by neural net\n",
    "\n",
    "    def simulate(self, env):\n",
    "        s = env.state()\n",
    "        obs = env.observe(env.agent_selection)\n",
    "        action_mask = obs['action_mask']\n",
    "        action_type = obs['action_type']\n",
    "        obs = obs['observation']\n",
    "\n",
    "        \"\"\" EVALUATE \"\"\"\n",
    "        # If s is leaf (different from terminal state), then evaluate s with model\n",
    "        if s not in self.p_s:\n",
    "            p, v = self.evaluate(obs, action_mask, action_type)\n",
    "            self.p_s[s] = p\n",
    "            return v  # This is NOT -v because sign doesn't necessarily change at every level.\n",
    "\n",
    "        \"\"\" SELECT \"\"\"\n",
    "        cpuct = 1\n",
    "        q = np.array([self.q_sa.get((s, a), 0) for a in range(len(action_mask))])\n",
    "        n = np.array([self.n_sa.get((s, a), 0) for a in range(len(action_mask))])\n",
    "\n",
    "        # (1− ε)pa+ εηa, where η∼ Dir(0.03) and ε= 0.25\n",
    "        p = 0.75 * self.p_s[s] + 0.25 * np.random.dirichlet([2] * len(q))\n",
    "\n",
    "        u = q + cpuct * p * np.sqrt(n.sum() + 1e-6) / (1 + n)\n",
    "        u[action_mask == 0] = -np.inf\n",
    "        a = u.argmax()\n",
    "\n",
    "        \"\"\" EXPAND \"\"\"\n",
    "        next_env = deepcopy(env)\n",
    "        next_env.step(a)\n",
    "        agent, next_agent = env.agent_selection, next_env.agent_selection\n",
    "        # If terminal state, v = z\n",
    "        if next_env.terminations[agent] or next_env.truncations[next_agent]:\n",
    "            v = next_env.rewards[agent]\n",
    "        else:\n",
    "            v = self.simulate(next_env)\n",
    "            if agent != next_agent:\n",
    "                v = -v  # Change sign only if next agent is opponent.\n",
    "\n",
    "        \"\"\" BACKTRACK \"\"\"\n",
    "        if (s, a) in self.q_sa:\n",
    "            # Recompute the average\n",
    "            sum_ = self.n_sa[(s, a)] * self.q_sa[(s, a)] + v\n",
    "            self.n_sa[(s, a)] += 1\n",
    "            self.q_sa[(s, a)] = sum_ / self.n_sa[(s, a)]\n",
    "        else:\n",
    "            self.q_sa[(s, a)] = v\n",
    "            self.n_sa[(s, a)] = 1\n",
    "        return v  # Again, this is NOT -v.\n",
    "\n",
    "    def evaluate(self, obs, action_mask, action_type):\n",
    "        obs = torch.from_numpy(obs).float().view(1, -1)  # (1, 48)\n",
    "\n",
    "        # Get predictions\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            p, v = self.model(obs.float().to(device), torch.LongTensor([action_type]).to(device))  # (1, 24), (1, 1)\n",
    "        p = p.cpu().numpy()[0]  # (24,)\n",
    "        v = v.cpu().numpy()[0][0]  # scalar\n",
    "\n",
    "        # Legal action masking.\n",
    "        p *= action_mask\n",
    "\n",
    "        # Normalizing probabilities\n",
    "        p_sum = p.sum()\n",
    "        if p_sum > 0:\n",
    "            p /= p_sum  # re-normalize\n",
    "        else:\n",
    "            print(\"All valid moves are masked, doing a workaround.\")\n",
    "            p = action_mask / action_mask.sum()\n",
    "\n",
    "        return p, v\n",
    "\n",
    "    def get_action_prob(self, env, temperature=1):\n",
    "        s = env.state()\n",
    "        p = [self.n_sa.get((s, a), 0) for a in range(len(self.p_s[s]))]\n",
    "        p = np.array(p, dtype=np.float32) ** 1 / temperature\n",
    "        p *= env.action_masks[env.agent_selection]\n",
    "        return p / p.sum()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T12:13:47.364103Z",
     "start_time": "2023-09-06T12:13:46.570816Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Helper functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def plot(iteration, losses, win_rates, lose_rates):\n",
    "    clear_output(True)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    ax1.set_title(f\"Loss. Iteration: {iteration}\")\n",
    "    ax1.plot(losses)\n",
    "    ax2.set_title(f'Win rate and lose rate')\n",
    "    ax2.plot(win_rates, label='Win Rate')\n",
    "    ax2.plot(lose_rates, label='Lose Rate')\n",
    "    ax2.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluate(env, model1, model2, n_episodes):\n",
    "    wins1, wins2 = 0, 0\n",
    "    for _ in tqdm(range(n_episodes), desc='Evaluating'):\n",
    "        env.reset()\n",
    "        mcts_ = MCTS(model1), MCTS(model2)\n",
    "        step = 0\n",
    "        while not (env.terminations[env.agent_selection] or env.truncations[env.agent_selection]):\n",
    "            step += 1\n",
    "            mcts = mcts_[int(step % 2 == 0)]\n",
    "\n",
    "            [mcts.simulate(env) for _ in range(10)]\n",
    "            p = mcts.get_action_prob(env)\n",
    "            action = np.random.choice(len(p), p=p)\n",
    "\n",
    "            env.step(action)\n",
    "        r = env.rewards[env.agents[0]]\n",
    "        wins1 += int(r > 1)\n",
    "        wins2 += int(r < 1)\n",
    "    return wins1 / n_episodes, wins2 / n_episodes\n",
    "\n",
    "\n",
    "def self_play(env, mcts: MCTS):\n",
    "    tuples = []\n",
    "    env.reset()\n",
    "    while not (env.terminations[env.agent_selection] or env.truncations[env.agent_selection]):\n",
    "        [mcts.simulate(env) for _ in range(100)]\n",
    "        p = mcts.get_action_prob(env)\n",
    "        action = np.random.choice(len(p), p=p)\n",
    "        obs = env.observe(env.agent_selection)\n",
    "        tuples.append([obs['observation'], obs['action_type'], p, env.agent_selection])\n",
    "\n",
    "        env.step(action)\n",
    "    for tup in tuples:\n",
    "        tup[-1] = env.rewards[tup[-1]]\n",
    "    return tuples"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T12:13:55.020470Z",
     "start_time": "2023-09-06T12:13:55.000174Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initialization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m env \u001B[38;5;241m=\u001B[39m nmm\u001B[38;5;241m.\u001B[39menv()\n\u001B[1;32m      3\u001B[0m device \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda:1\u001B[39m\u001B[38;5;124m'\u001B[39m  \u001B[38;5;66;03m# 'cpu'\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAlphaZeroNMM\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m old_model \u001B[38;5;241m=\u001B[39m deepcopy(model)\n\u001B[1;32m      6\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m Adam(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-5\u001B[39m, weight_decay\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-6\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/gyms2/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1145\u001B[0m, in \u001B[0;36mModule.to\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1141\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1142\u001B[0m                     non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[1;32m   1143\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, non_blocking)\n\u001B[0;32m-> 1145\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/gyms2/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:797\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    795\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[1;32m    796\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 797\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    799\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    800\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    801\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    802\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    807\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    808\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/gyms2/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:797\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    795\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[1;32m    796\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 797\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    799\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    800\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    801\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    802\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    807\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    808\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/gyms2/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:820\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    816\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[1;32m    817\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[1;32m    818\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[1;32m    819\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 820\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    821\u001B[0m should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[1;32m    822\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m should_use_set_data:\n",
      "File \u001B[0;32m~/PycharmProjects/gyms2/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1143\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[0;34m(t)\u001B[0m\n\u001B[1;32m   1140\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n\u001B[1;32m   1141\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1142\u001B[0m                 non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[0;32m-> 1143\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/gyms2/venv/lib/python3.8/site-packages/torch/cuda/__init__.py:239\u001B[0m, in \u001B[0;36m_lazy_init\u001B[0;34m()\u001B[0m\n\u001B[1;32m    235\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    236\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    237\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    238\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m--> 239\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    240\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    241\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[1;32m    242\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "env = nmm.env()\n",
    "\n",
    "device = 'cuda:1'  # 'cpu'\n",
    "model = AlphaZeroNMM().to(device)\n",
    "old_model = deepcopy(model)\n",
    "optimizer = Adam(model.parameters(), lr=1e-5, weight_decay=1e-6)\n",
    "\n",
    "losses = []\n",
    "win_rates = []\n",
    "lose_rates = []\n",
    "dataset = deque([], maxlen=50000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T12:13:58.949335Z",
     "start_time": "2023-09-06T12:13:58.453723Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for iteration in range(500):\n",
    "    \"\"\" Self Play \"\"\"\n",
    "    for _ in tqdm(range(25), desc='Self-Play'):\n",
    "        mcts = MCTS(model)\n",
    "        dataset += self_play(env, mcts)\n",
    "\n",
    "    \"\"\" Train \"\"\"\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(10), desc='Training'):\n",
    "        loss_mean = MeanMetric()\n",
    "        for state, action_type, p, v in DataLoader(dataset, batch_size=16, shuffle=True):\n",
    "            p_pred, v_pred = model(state.float().to(device), torch.LongTensor(action_type).to(device))\n",
    "            loss = (v.to(device) - v_pred).pow(2).mean() - (p.to(device) * p_pred.log()).mean()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_mean(loss.item())\n",
    "        losses.append(loss_mean.compute())\n",
    "\n",
    "    \"\"\" Evaluate \"\"\"\n",
    "    wins_0, loses_0 = evaluate(env, model, old_model, 10)\n",
    "    loses_1, wins_1 = evaluate(env, old_model, model, 10)\n",
    "\n",
    "    win_rates.append((wins_0 + wins_1) / 2)\n",
    "    lose_rates.append((loses_0 + loses_1) / 2)\n",
    "\n",
    "    # if iteration % 5 == 0:\n",
    "    #     old_model = deepcopy(model)\n",
    "    if win_rates[-1] > lose_rates[-1]:\n",
    "        old_model = deepcopy(model)\n",
    "    # else:\n",
    "    #     model = deepcopy(old_model)\n",
    "\n",
    "    plot(iteration, losses, win_rates, lose_rates)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), '/Users/akhildevarashetti/code/reward_lab/exp/nmm/weights/model_3.pth')\n",
    "torch.save(model.state_dict(), '/home/akhil/code/reward_lab/exp/nmm/weights/model_2.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sample Gameplay"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env.reset()\n",
    "mcts = MCTS(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "[mcts.simulate(env) for _ in range(25)]\n",
    "p = mcts.get_action_prob(env)\n",
    "# action = np.random.choice(len(p), p=p)\n",
    "action = p.argmax()\n",
    "for i, pi in enumerate(p):\n",
    "    print(f'{i}:\\t{pi}')\n",
    "print(f'{action=}\\n{env.agent_selection=}\\n{env.get_action_type(env.agent_selection)}')\n",
    "\n",
    "print(env.render())\n",
    "env.step(action)\n",
    "print(f'{env.rewards=}')\n",
    "print(env.render())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
